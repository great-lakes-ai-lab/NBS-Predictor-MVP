{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/Pre-process CFS forecast data\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/28/2024\n",
    "Updated: 10/09/2024\n",
    "\n",
    "This script:\n",
    "1. Downloads CFS forecast data from the AWS as grib2 files. \n",
    "2. Opens the grib2 files, calculates total basin, lake, and land, precipitation, evaporation, and average 2m air temperature. \n",
    "3. These calculations are then added to the CSV files. \n",
    "\n",
    "This script needs the following files:\n",
    "\n",
    "- input\n",
    "    - GL_mask.nc\n",
    "\n",
    "Optional:\n",
    "- data\n",
    "    - CFS_EVAP_forecasts_Sums_CMS.csv\n",
    "    - CFS_PCP_forecasts_Sums_CMS.csv\n",
    "    - CFS_TMP_forecasts_Avgs_K.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import cfgrib\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to download data to\n",
    "#dir = 'C:/Users/fitzpatrick/Desktop/NBS-Predictor-MVP-main/notebooks/production'\n",
    "dir = './'\n",
    "# Location of the mask file\n",
    "mask_file = dir + '/input/GL_mask.nc'\n",
    "\n",
    "# Location of existing CSV files or path/name to new CSV files\n",
    "tmp_csv = dir + '/data/CFS_TMP_forecasts_Avgs_K.csv'\n",
    "evap_csv = dir + '/data/CFS_EVAP_forecasts_Sums_CMS.csv'\n",
    "pcp_csv = dir + '/data/CFS_PCP_forecasts_Sums_CMS.csv'\n",
    "\n",
    "# IF YOU ARE CREATING NEW CSV FILES:\n",
    "# Then you need to define the start and end dates\n",
    "# IF YOU ARE ADDING TO EXISTING CSV FILES:\n",
    "# Then these dates will be ignored and the script will automatically pull\n",
    "# the last date from the existing CSV files and continue the forecast from there.\n",
    "start_date = '2024-08-31'\n",
    "end_date = '2024-08-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presets\n",
    "\n",
    "These shouldn't change unless the location changes for CFS data or the user wants different files (products specifies the prefix of the files. Different files contain different variables) or a specific forecast (utc specifies the forecast time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Presets ##\n",
    "products = ['pgb','flx']\n",
    "utc = ['00','06','12','18']\n",
    "\n",
    "# Define mask variables\n",
    "mask_variables = ['eri_basin','eri_lake','eri_land',\n",
    "                 'hur_basin','hur_lake','hur_land',\n",
    "                 'ont_basin','ont_lake','ont_land',\n",
    "                 'mic_basin','mic_lake','mic_land',\n",
    "                 'sup_basin','sup_lake','sup_land']\n",
    "\n",
    "#AWS bucket name to locate the CFS forecast\n",
    "bucket_name = 'noaa-cfs-pds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "This function goes to the AWS site and downloads the needed CFS files for a given forecast day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_grb2_aws(product, bucket_name, folder_path, download_dir):\n",
    "    \"\"\"\n",
    "    Download the CFS forecast from AWS\n",
    "\n",
    "    Parameters:\n",
    "    - product: 'flx' or 'pgb'\n",
    "    - bucket_name: for CFS data it is 'noaa-cfs-pds'\n",
    "    - folder_path: the url path to data\n",
    "    - download_dir: location to download data to\n",
    "    \"\"\"\n",
    "    num_files_downloaded = 0\n",
    "\n",
    "    # Create a boto3 client for S3\n",
    "    s3_config = Config(signature_version=UNSIGNED)\n",
    "    s3 = boto3.client('s3', config=s3_config)\n",
    "\n",
    "    # List all objects in the specified folder path\n",
    "    continuation_token = None\n",
    "    objects = []\n",
    "\n",
    "    # Use a loop to handle pagination\n",
    "    while True:\n",
    "        list_objects_args = {'Bucket': bucket_name, 'Prefix': folder_path}\n",
    "        if continuation_token:\n",
    "            list_objects_args['ContinuationToken'] = continuation_token\n",
    "\n",
    "        list_objects_response = s3.list_objects_v2(**list_objects_args)\n",
    "\n",
    "        objects.extend(list_objects_response.get('Contents', []))\n",
    "\n",
    "        if not list_objects_response.get('IsTruncated', False):\n",
    "            break\n",
    "\n",
    "        continuation_token = list_objects_response.get('NextContinuationToken')\n",
    "\n",
    "    # Iterate over each object and download if it ends with '.grb2'\n",
    "    for obj in objects:\n",
    "        key = obj['Key']\n",
    "        if product in key and key.endswith('grib.grb2'): #if key.endswith('.grb2'):\n",
    "            local_file_path = os.path.join(download_dir, os.path.relpath(key, folder_path))\n",
    "\n",
    "            # Ensure the directory structure exists\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "            # Download the file\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "            num_files_downloaded += 1\n",
    "\n",
    "            print(f\"Downloaded: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataframes(tmp_csv, evap_csv, pcp_csv):\n",
    "    \"\"\"\n",
    "    Initialize new DataFrames if CSV files do not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(tmp_csv):\n",
    "        df_tmp_forecasts = pd.read_csv(tmp_csv)\n",
    "    else:\n",
    "        df_tmp_forecasts = pd.DataFrame(columns=['cfs_run', 'forecast_year', 'forecast_month'] + mask_variables)\n",
    "    \n",
    "    if os.path.exists(evap_csv):\n",
    "        df_evap_forecasts = pd.read_csv(evap_csv)\n",
    "    else:\n",
    "        df_evap_forecasts = pd.DataFrame(columns=['cfs_run', 'forecast_year', 'forecast_month'] + mask_variables)\n",
    "    \n",
    "    if os.path.exists(pcp_csv):\n",
    "        df_pcp_forecasts = pd.read_csv(pcp_csv)\n",
    "    else:\n",
    "        df_pcp_forecasts = pd.DataFrame(columns=['cfs_run', 'forecast_year', 'forecast_month'] + mask_variables)\n",
    "    \n",
    "    return df_tmp_forecasts, df_evap_forecasts, df_pcp_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to grab a specific list of files based on the prefix or suffix of a file (ie. 'pgb', '.grb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory, affix, identifier):\n",
    "    \"\"\"\n",
    "    Get a list of all GRIB2 files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory containing files.\n",
    "    - affix (str): 'prefix' or 'suffix'\n",
    "    - identifier (str):  (ie. 'pgb', 'flx', '.grb2', or '.nc')\n",
    "    Returns:\n",
    "    - List of file paths to the GRIB2 files.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if affix == 'suffix': # ends with\n",
    "            if file_name.endswith(identifier):\n",
    "                file_path = os.path.join(directory, file_name)\n",
    "                files.append(file_path)\n",
    "        elif affix == 'prefix': # begins with\n",
    "            if file_name.startswith(identifier):\n",
    "                file_path = os.path.join(directory, file_name)\n",
    "                files.append(file_path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to delete the directory with CFS grb2 files because they are not needed after calculations are saved in the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory(directory_path):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "    try:\n",
    "        # Remove the entire directory tree\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"Successfully deleted the directory and all its contents: {directory_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {directory_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the grid cell areas [m2] based on the mask file. This is needed to calculate total precipitation and evaporation because the units are [kg/m2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_cell_areas(lon, lat):\n",
    "    # Calculate grid cell areas\n",
    "    # Assuming lat and lon are 1D arrays\n",
    "    # Convert latitude to radians\n",
    "\n",
    "    R = 6371000.0  # Radius of Earth in meters\n",
    "    lat_rad = np.radians(lat)\n",
    "\n",
    "    # Calculate grid cell width in radians\n",
    "    dlat = np.radians(lat[1] - lat[0])\n",
    "    dlon = np.radians(lon[1] - lon[0])\n",
    "\n",
    "    # Calculate area of each grid cell in square kilometers\n",
    "    area = np.zeros((len(lat), len(lon)))\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            area[i, j] = R**2 * dlat * dlon * np.cos(lat_rad[i])\n",
    "\n",
    "    return area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate evaporation based on the 2m air temperature and latent heat flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET = kg/(m^2*time^1) or 1 mm\n",
    "# LE = MJ/(M^2*time^1)\n",
    "# λ  = MJ/kg\n",
    "\n",
    "# Latent heat of vaporization varies slightly with temperature. Allen et al. (1998) provides an equation \n",
    "# for calculating λ with air  temperature variation. Temperature in this case must be in degrees Celcius.\n",
    "\n",
    "# λ=2.501−(2.361×10−3)×Temp Celcius\n",
    "\n",
    "# so for our data with Temp in Kelvin...\n",
    "\n",
    "# λ=2.501−((2.361×10−3)×(Temp-273.15))\n",
    "\n",
    "# Our variable_lhf is in W/m^2 or J/(m^2*time^1). In order to convert to MJ we must multiply by 10^-6 or \n",
    "# 0.000001. Now we have lamba and variable_lhf both in terms of MJ.\n",
    "\n",
    "# Equation below will provide an evaporation rate in kg/m2 per s. \n",
    "\n",
    "def calculate_evaporation(temperature_K, latent_heat):\n",
    "    lamda=(2.501-(0.002361*(temperature_K-273.15)))\n",
    "    evaporation_rate=((latent_heat)*0.000001)/lamda\n",
    "\n",
    "    return evaporation_rate # kg/m2 per s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to open each of the grib2 files and calculate the total precipitation, total evaporation, and average 2m air temperature over an entire basin, land, or lake for each of the Great Lakes. This uses the mask file to calculate each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grib_files(download_dir, df_tmp_forecasts, df_evap_forecasts, df_pcp_forecasts, mask_lat, mask_lon, mask_ds, mask_variables, area, calculate_evaporation):\n",
    "    # Find all the .grb2 files in the directory\n",
    "    file_list = get_files(download_dir, 'suffix', '.grb2')\n",
    "    index = len(df_tmp_forecasts) if not df_tmp_forecasts.empty else 0  # Picks up on the last line of the CSV\n",
    "\n",
    "    for grib2_file in file_list:\n",
    "\n",
    "        filename = os.path.basename(grib2_file)\n",
    "        parts = filename.split('.')\n",
    "        cfs_run = parts[2]\n",
    "        date_part = parts[3]  # Assuming parts[2] is in the format YYYYMM\n",
    "        forecast_year = date_part[:4]\n",
    "        forecast_month = date_part[4:6]\n",
    "\n",
    "        if filename.startswith('flxf'):\n",
    "\n",
    "            # Open the flx file at the 2m level to pull the 2m air temperature\n",
    "            flx_2mabove = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'heightAboveGround', 'level': 2})\n",
    "            df_tmp_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
    "            df_tmp_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
    "            df_tmp_forecasts.loc[index, 'forecast_month'] = forecast_month\n",
    "            mean2t = flx_2mabove['mean2t']\n",
    "\n",
    "            # Cut the variable to the mask domain\n",
    "            mean2t_cut = mean2t.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            mean2t_remap = mean2t_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "            \n",
    "            # Calculate mean2t for each of the mask variables (i.e., eri_lake, eri_basin, etc.)\n",
    "            for mask_var in mask_variables:\n",
    "\n",
    "                mask = mask_ds.variables[mask_var][:]\n",
    "                # Take the mean over the mask area\n",
    "                tmp_avg = np.mean(mean2t_remap * mask)\n",
    "\n",
    "                df_tmp_forecasts.loc[index, mask_var] = tmp_avg.data\n",
    "\n",
    "            ###############################################################################\n",
    "\n",
    "            # Open the flx file again but at the surface level to pull the latent heat flux\n",
    "            flx_surface = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'})\n",
    "            df_evap_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
    "            df_evap_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
    "            df_evap_forecasts.loc[index, 'forecast_month'] = forecast_month\n",
    "            mslhf = flx_surface['mslhf']\n",
    "            \n",
    "            # Cut the variable to the mask domain\n",
    "            mslhf_cut = mslhf.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            mslhf_remap = mslhf_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "            \n",
    "            # Calculate evaporation across the entire domain using air temp and latent heat flux\n",
    "            evap = calculate_evaporation(mean2t_remap, mslhf_remap)\n",
    "            \n",
    "            # Calculate evaporation for each of the mask variables (i.e., eri_lake, eri_basin, etc.)\n",
    "            for mask_var in mask_variables:\n",
    "                \n",
    "                mask = mask_ds.variables[mask_var][:]\n",
    "                total_evap = (np.sum(evap * area * mask)) # Converts kg/s/m2 to kg/s\n",
    "                # Convert kg/s to m³/s (assuming density of water ≈ 1000 kg/m³)\n",
    "                evap_cms = total_evap / 1000.0\n",
    "\n",
    "                df_evap_forecasts.loc[index, mask_var] = evap_cms.data\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        elif filename.startswith('pgbf'):\n",
    "\n",
    "            # Open the pgb file at the surface level to pull the precipitation\n",
    "            pgb_surface = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'})\n",
    "            df_pcp_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
    "            df_pcp_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
    "            df_pcp_forecasts.loc[index, 'forecast_month'] = forecast_month\n",
    "            pcp = pgb_surface['tp']  # Total precipitation\n",
    "            \n",
    "            # Cut the variable to the mask domain\n",
    "            pcp_cut = pcp.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            pcp_remap = pcp_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "            \n",
    "            for mask_var in mask_variables:\n",
    "                mask = mask_ds.variables[mask_var][:]\n",
    "                \n",
    "                # Convert precipitation from kg/m² per 6 hours to kg/m² per second\n",
    "                pcp_per_s = pcp_remap / 21600.0 # seconds in 6hrs\n",
    "                total_pcp_kg_per_s = (np.sum(pcp_per_s * area * mask)) # kg/s\n",
    "\n",
    "                # Convert kg/s to m³/s (assuming density of water ≈ 1000 kg/m³)\n",
    "                total_pcp_cms = total_pcp_kg_per_s / 1000.0\n",
    "                df_pcp_forecasts.loc[index, mask_var] = total_pcp_cms.data\n",
    "\n",
    "        print(f'Done with {filename}')\n",
    "\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the mask file. Pull the latitude and longitude to be used to cut the global variable down to just the Great Lakes domain and upscale. Also calculates area of each of the grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: 2024-10-09 and continuing through: 2024-10-09\n"
     ]
    }
   ],
   "source": [
    "# Open existing CSVs or create empty dataframes to save to new CSVs\n",
    "df_tmp_forecasts, df_evap_forecasts, df_pcp_forecasts = initialize_dataframes(tmp_csv, evap_csv, pcp_csv)\n",
    "\n",
    "# If we are starting a new CSV, then user must input dates above to pull data\n",
    "if df_tmp_forecasts.empty:\n",
    "    print(\"Creating new files.\")\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\") # User input above\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\") # User input above\n",
    "else:\n",
    "    # If we are adding to an existing CSV, then pull the last date from the CSV\n",
    "    # and continue from there\n",
    "    last_cfs = df_tmp_forecasts['cfs_run'].astype(str).iloc[-1][:8]\n",
    "    start_date = datetime.strptime(last_cfs, '%Y%m%d') + timedelta(days=1)\n",
    "    # Pull all the forecasts days up to yesterday (the most complete forecast)\n",
    "    end_date = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Check if start_date is equal to or after end_date\n",
    "if start_date > end_date:\n",
    "    print(\"The files are up-to-date.\")\n",
    "    sys.exit()  # Stop the script\n",
    "\n",
    "print(f\"Starting from: {start_date.strftime('%Y-%m-%d')} and continuing through: {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create a date range\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "# Convert to integer format YYYYMMDD\n",
    "dates_array = date_range.strftime('%Y%m%d').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the mask file and calculate the grid cell areas\n",
    "mask_ds = nc.Dataset(mask_file)\n",
    "mask_lat = mask_ds.variables['latitude'][:]\n",
    "mask_lon = mask_ds.variables['longitude'][:]\n",
    "area = calculate_grid_cell_areas(mask_lon, mask_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin loop to go through the user input dates. Loop creates a directory to download the CFS grib files, runs through the download_grb2_aws funtion to download and then run through the process_grib_files to do the calculations. It then saves the calculations to the CSV files, deletes the grib2 files and moves on to the next date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 20241009.\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/pgbf.01.2024100900.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/00/monthly_grib_01/flxf.01.2024100900.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/pgbf.01.2024100906.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/06/monthly_grib_01/flxf.01.2024100906.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/pgbf.01.2024100912.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/12/monthly_grib_01/flxf.01.2024100912.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/pgbf.01.2024100918.202507.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202410.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202411.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202412.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202501.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202502.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202503.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202504.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202505.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202506.avrg.grib.grb2\n",
      "Downloaded: cfs.20241009/18/monthly_grib_01/flxf.01.2024100918.202507.avrg.grib.grb2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024100900' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_tmp_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_tmp_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '10' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_tmp_forecasts.loc[index, 'forecast_month'] = forecast_month\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024100900' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_evap_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:46: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_evap_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '10' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_evap_forecasts.loc[index, 'forecast_month'] = forecast_month\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with flxf.01.2024100900.202410.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202411.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202412.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202501.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202502.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202503.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202504.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202505.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202506.avrg.grib.grb2\n",
      "Done with flxf.01.2024100900.202507.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202410.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202411.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202412.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202501.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202502.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202503.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202504.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202505.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202506.avrg.grib.grb2\n",
      "Done with flxf.01.2024100906.202507.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202410.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202411.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202412.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202501.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202502.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202503.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202504.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202505.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202506.avrg.grib.grb2\n",
      "Done with flxf.01.2024100912.202507.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202410.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202411.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202412.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202501.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202502.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202503.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202504.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202505.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202506.avrg.grib.grb2\n",
      "Done with flxf.01.2024100918.202507.avrg.grib.grb2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024100900' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_pcp_forecasts.loc[index, 'cfs_run'] = cfs_run\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:78: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_pcp_forecasts.loc[index, 'forecast_year'] = forecast_year\n",
      "C:\\Users\\fitzpatrick\\AppData\\Local\\Temp\\1\\ipykernel_8132\\4153316585.py:79: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '10' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_pcp_forecasts.loc[index, 'forecast_month'] = forecast_month\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with pgbf.01.2024100900.202410.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202411.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202412.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202501.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202502.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202503.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202504.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202505.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202506.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100900.202507.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202410.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202411.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202412.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202501.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202502.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202503.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202504.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202505.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202506.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100906.202507.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202410.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202411.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202412.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202501.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202502.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202503.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202504.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202505.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202506.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100912.202507.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202410.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202411.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202412.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202501.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202502.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202503.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202504.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202505.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202506.avrg.grib.grb2\n",
      "Done with pgbf.01.2024100918.202507.avrg.grib.grb2\n",
      "Done with 20241009.\n"
     ]
    }
   ],
   "source": [
    "for date in dates_array:\n",
    "    print(f\"Beginning {date}.\")\n",
    "    download_dir = f'{dir}/data/{date}/'\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Uses AWS to download the grib2 files\n",
    "    for utc_time in utc:\n",
    "        for product in products:\n",
    "            folder_path = f'cfs.{date}/{utc_time}/monthly_grib_01/'\n",
    "            download_grb2_aws(product, bucket_name, folder_path, download_dir)\n",
    "\n",
    "    process_grib_files(download_dir, df_tmp_forecasts, df_evap_forecasts, df_pcp_forecasts, mask_lat, mask_lon, mask_ds, mask_variables, area, calculate_evaporation)   \n",
    "    \n",
    "    # Save the updated DataFrames to CSV files\n",
    "    df_tmp_forecasts.to_csv(tmp_csv, sep=',', index=False)\n",
    "    df_evap_forecasts.to_csv(evap_csv, sep=',', index=False)\n",
    "    df_pcp_forecasts.to_csv(pcp_csv, sep=',', index=False)\n",
    "\n",
    "    # Delete downloaded grib2 files\n",
    "    #delete_directory(download_dir)\n",
    "    \n",
    "    print(f\"Done with {date}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close any open files before finishing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
