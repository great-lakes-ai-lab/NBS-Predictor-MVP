{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/19/2024\n",
    "\n",
    "This script reads in CFSR data from 1979 - 2010 and trains different machine learning\n",
    "models to target RNBS from GLCC across the 5 Great Lakes simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, RBF, Matern, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where the CFSR and GLCC files are located\n",
    "dir = 'C:/Users/fitzpatrick/Desktop/Data/Input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_in_month(year, month):\n",
    "    # Number of days in the month\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    # Convert days to seconds\n",
    "    return num_days * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_kg_to_cms_cfsr(df):\n",
    "\n",
    "    # Calculate the number of seconds for each month\n",
    "    df['seconds'] = df.apply(lambda row: seconds_in_month(int(row['year']), int(row['month'])), axis=1)\n",
    "\n",
    "    # Convert kg to meters cubed and divide by the seconds in the month\n",
    "    df['WaterErie_cms'] = (df['WaterErie'] / 1000) / df['seconds']\n",
    "    df['WaterOntario_cms'] = (df['WaterOntario'] / 1000) / df['seconds']\n",
    "    df['WaterSuperior_cms'] = (df['WaterSuperior'] / 1000) / df['seconds']\n",
    "    df['WaterMichHuron_cms'] = ((df['WaterMichigan'] + df['WaterHuron']) / 1000) / df['seconds']\n",
    "    df['LandErie_cms'] = (df['LandErie'] / 1000) / df['seconds']\n",
    "    df['LandOntario_cms'] = (df['LandOntario'] / 1000) / df['seconds']\n",
    "    df['LandSuperior_cms'] = (df['LandSuperior'] / 1000) / df['seconds']\n",
    "    df['LandMichHuron_cms'] = ((df['LandMichigan'] + df['LandHuron']) / 1000) / df['seconds']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mm_to_cms_l2(df, lake):\n",
    "\n",
    "    # Define the lake surface areas directly within the function\n",
    "    lake_sa_dict = {\n",
    "        'Superior': 82097*1000000,\n",
    "        'MichHuron': (57753 + 5956)*1000000,\n",
    "        'Erie': 25655*1000000,\n",
    "        'Ontario': 19009*1000000\n",
    "    }\n",
    "    \n",
    "    # Get the surface area for the specified lake\n",
    "    lake_sa = lake_sa_dict.get(lake, None)\n",
    "    \n",
    "    # Check if the lake surface area was found\n",
    "    if lake_sa is None:\n",
    "        raise ValueError(f\"Lake '{lake}' is not recognized. Please provide a valid lake name, either 'Superior', 'Erie', 'Ontario', or 'MichHuron'.\")\n",
    "    \n",
    "    # Calculate the number of seconds for each month\n",
    "    df['seconds'] = df.apply(lambda row: seconds_in_month(int(row['Year']), int(row['Month'])), axis=1)\n",
    "\n",
    "    # Convert millimeters to meters cubed and divide by seconds\n",
    "    df['Median_cms'] = (df['Median'] / 1000) / df['seconds'] * lake_sa\n",
    "    df['2.5_cms'] = (df['2.5 Percentile'] / 1000) / df['seconds'] * lake_sa\n",
    "    df['97.5_cms'] = (df['97.5 Percentile'] / 1000) / df['seconds'] * lake_sa\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_variables(df, lags):\n",
    "    \"\"\"\n",
    "    Create lagged variables for specified columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - columns (list of str): The list of column names for which to create lagged variables.\n",
    "    - lags (int): The number of lagged variables to create.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the original columns and the new lagged variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()  # To avoid modifying the original DataFrame\n",
    "    \n",
    "    for column in df.columns:\n",
    "        for lag in range(1, lags + 1):\n",
    "            df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "\n",
    "    # Drop rows with any NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in PCP data from CFSR [kg]\n",
    "data_1 = pd.read_csv(dir+'CFSR_APCP_Basin_Sums.csv',sep=',')\n",
    "\n",
    "## Read in EVAP data from CFSR [kg]\n",
    "data_2 = pd.read_csv(dir+'CFSR_EVAP_Basin_Sums.csv',sep=',')\n",
    "\n",
    "## Read in TMP data from CFSR [K]\n",
    "data_3 = pd.read_csv(dir+'CFSR_TMP_Basin_Avgs.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Total Precipitation to cms\n",
    "data_1 = convert_kg_to_cms_cfsr(data_1)\n",
    "\n",
    "# Convert Total Evaporation to cms\n",
    "data_2 = convert_kg_to_cms_cfsr(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the GLCC RNBS CSVs file in [cms]\n",
    "\n",
    "https://www.greatlakescc.org/en/coordinating-committee-products-and-datasets/#:~:text=Monthly%20Residual%20Net%20Basin%20Supplies%3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_evap_file = pd.read_csv(dir + 'SupEvap_analysis19502022_prior19001969_1m.csv')\n",
    "sup_runoff_file = pd.read_csv(dir + 'SupRunoff_analysis19502022_prior19001969_1m.csv')\n",
    "sup_precip_file = pd.read_csv(dir + 'SupPrecip_analysis19502022_prior19001969_1m.csv')\n",
    "\n",
    "eri_evap_file = pd.read_csv(dir + 'ErieEvap_analysis19502022_prior19001969_1m.csv')\n",
    "eri_runoff_file = pd.read_csv(dir + 'ErieRunoff_analysis19502022_prior19001969_1m.csv')\n",
    "eri_precip_file = pd.read_csv(dir + 'EriePrecip_analysis19502022_prior19001969_1m.csv')\n",
    "\n",
    "ont_evap_file = pd.read_csv(dir + 'OntEvap_analysis19502022_prior19001969_1m.csv')\n",
    "ont_runoff_file = pd.read_csv(dir + 'OntRunoff_analysis19502022_prior19001969_1m.csv')\n",
    "ont_precip_file = pd.read_csv(dir + 'OntPrecip_analysis19502022_prior19001969_1m.csv')\n",
    "\n",
    "mih_evap_file = pd.read_csv(dir + 'MiHurEvap_analysis19502022_prior19001969_1m.csv')\n",
    "mih_runoff_file = pd.read_csv(dir + 'MiHurRunoff_analysis19502022_prior19001969_1m.csv')\n",
    "mih_precip_file = pd.read_csv(dir + 'MiHurPrecip_analysis19502022_prior19001969_1m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year  Month      Median  2.5 Percentile  97.5 Percentile\n",
      "0    1950.0    1.0   67.245050       51.865840        84.204657\n",
      "1    1950.0    2.0   35.728114       20.270061        50.587482\n",
      "2    1950.0    3.0   12.926938       -2.109247        28.563223\n",
      "3    1950.0    4.0   12.655983        3.428071        22.222697\n",
      "4    1950.0    5.0   -3.229512       -9.853121         3.477014\n",
      "..      ...    ...         ...             ...              ...\n",
      "863  2021.0   12.0  112.903571       99.068128       126.926402\n",
      "864  2022.0    1.0  139.083649      125.006839       153.776202\n",
      "865  2022.0    2.0   68.063946       54.649119        81.100302\n",
      "866  2022.0    3.0   39.705374       26.086656        53.065299\n",
      "867  2022.0    4.0   12.252169        3.359856        21.576276\n",
      "\n",
      "[868 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sup_evap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_evap = convert_mm_to_cms_l2(sup_evap_file, 'Superior')\n",
    "sup_runoff = convert_mm_to_cms_l2(sup_runoff_file, 'Superior')\n",
    "sup_precip = convert_mm_to_cms_l2(sup_precip_file, 'Superior')\n",
    "\n",
    "eri_evap = convert_mm_to_cms_l2(eri_evap_file, 'Erie')\n",
    "eri_runoff = convert_mm_to_cms_l2(eri_runoff_file, 'Erie')\n",
    "eri_precip = convert_mm_to_cms_l2(eri_precip_file, 'Erie')\n",
    "\n",
    "ont_evap = convert_mm_to_cms_l2(ont_evap_file, 'Ontario')\n",
    "ont_runoff = convert_mm_to_cms_l2(ont_runoff_file, 'Ontario')\n",
    "ont_precip = convert_mm_to_cms_l2(ont_precip_file, 'Ontario')\n",
    "\n",
    "mih_evap = convert_mm_to_cms_l2(mih_evap_file, 'MichHuron')\n",
    "mih_runoff = convert_mm_to_cms_l2(mih_runoff_file, 'MichHuron')\n",
    "mih_precip = convert_mm_to_cms_l2(mih_precip_file, 'MichHuron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year  Month    Median  2.5 Percentile  97.5 Percentile  seconds  \\\n",
      "0    1950.0    1.0   74.9800         61.8440          87.8480  2678400   \n",
      "1    1950.0    2.0   47.2140         35.0710          58.9410  2419200   \n",
      "2    1950.0    3.0   34.2850         21.9170          46.7320  2678400   \n",
      "3    1950.0    4.0   13.0790          4.9639          21.0750  2592000   \n",
      "4    1950.0    5.0   -1.3051         -7.6169           5.3237  2678400   \n",
      "..      ...    ...       ...             ...              ...      ...   \n",
      "863  2021.0   12.0  101.9500         91.1620         113.0600  2678400   \n",
      "864  2022.0    1.0   97.0030         86.1040         107.4900  2678400   \n",
      "865  2022.0    2.0   50.9510         40.9880          60.8990  2419200   \n",
      "866  2022.0    3.0   28.0770         14.6670          41.8310  2678400   \n",
      "867  2022.0    4.0    5.2800         -2.7979          14.0110  2592000   \n",
      "\n",
      "      Median_cms      2.5_cms     97.5_cms  \n",
      "0    1783.490450  1471.034721  2089.571473  \n",
      "1    1243.368356   923.585623  1552.195837  \n",
      "2     815.510404   521.322488  1111.577430  \n",
      "3     321.469912   122.008142   518.004311  \n",
      "4     -31.043390  -181.177226   126.630676  \n",
      "..           ...          ...          ...  \n",
      "863  2425.004686  2168.398991  2689.269542  \n",
      "864  2307.334277  2048.088312  2556.780320  \n",
      "865  1341.781274  1079.408272  1603.759256  \n",
      "866   667.845577   348.872425   995.001187  \n",
      "867   129.777593   -68.769835   344.377623  \n",
      "\n",
      "[868 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mih_evap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and testing. We set the features 'X' as total over lake\n",
    "precipitation, total over lake evaporation, and the average air temperature over each lake. The\n",
    "targets 'y' are RNBS for each lake simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = pd.DataFrame({\n",
    "    'su_pcp_w': data_1['WaterSuperior_cms'],\n",
    "    'er_pcp_w': data_1['WaterErie_cms'],\n",
    "    'on_pcp_w': data_1['WaterOntario_cms'],\n",
    "    'mh_pcp_w': data_1['WaterMichHuron_cms'], #data_1['WaterMichigan']+data_1['WaterHuron'], # add the sums\n",
    "    'su_pcp_l': data_1['LandSuperior_cms'],\n",
    "    'er_pcp_l': data_1['LandErie_cms'],\n",
    "    'on_pcp_l': data_1['LandOntario_cms'],\n",
    "    'mh_pcp_l': data_1['LandMichHuron_cms'],\n",
    "    'su_evap_w': data_2['WaterSuperior_cms'],\n",
    "    'er_evap_w': data_2['WaterErie_cms'],\n",
    "    'on_evap_w': data_2['WaterOntario_cms'],\n",
    "    'mh_evap_w': data_2['WaterMichHuron_cms'], #data_2['WaterMichigan']+data_2['WaterHuron'], # add the sums\n",
    "    'su_evap_l': data_2['LandSuperior_cms'],\n",
    "    'er_evap_l': data_2['LandErie_cms'],\n",
    "    'on_evap_l': data_2['LandOntario_cms'],\n",
    "    'mh_evap_l': data_2['LandMichHuron_cms'],\n",
    "    'su_tmp_w': data_3['WaterSuperior'],\n",
    "    'er_tmp_w': data_3['WaterErie'],\n",
    "    'on_tmp_w': data_3['WaterOntario'],\n",
    "    'mh_tmp_w': (data_3['WaterMichigan']+data_3['WaterHuron'])/2,\n",
    "    'su_tmp_l': data_3['LandSuperior'],\n",
    "    'er_tmp_l': data_3['LandErie'],\n",
    "    'on_tmp_l': data_3['LandOntario'],\n",
    "    'mh_tmp_l': (data_3['LandMichigan']+data_3['LandHuron'])/2 # take the average temp\n",
    "})\n",
    "\n",
    "# Set the index by date\n",
    "X.set_index(pd.to_datetime(data_1[['year', 'month']].assign(day=1)), inplace=True)\n",
    "\n",
    "# Targets are the components of NBS (P, E, R)\n",
    "targets = pd.DataFrame({\n",
    "    'su_evap_y': sup_evap['Median_cms'],\n",
    "    'su_precip': sup_precip['Median_cms'],\n",
    "    'su_runoff': sup_runoff['Median_cms'],\n",
    "    'er_evap_y': eri_evap['Median_cms'],\n",
    "    'er_precip': eri_precip['Median_cms'],\n",
    "    'er_runoff': eri_runoff['Median_cms'],\n",
    "    'on_evap_y': ont_evap['Median_cms'],\n",
    "    'on_precip': ont_precip['Median_cms'],\n",
    "    'on_runoff': ont_runoff['Median_cms'],\n",
    "    'mh_evap_y': mih_evap['Median_cms'],\n",
    "    'mh_precip': mih_precip['Median_cms'],\n",
    "    'mh_runoff': mih_runoff['Median_cms'],\n",
    "})\n",
    "\n",
    "# Set the index of the targets\n",
    "targets.set_index(pd.to_datetime(eri_evap[['Year', 'Month']].assign(day=1)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               su_pcp_w    er_pcp_w    on_pcp_w     mh_pcp_w     su_pcp_l  \\\n",
      "1979-04-01  1833.850353  771.898492  361.687857  3439.840263  3900.078255   \n",
      "1979-05-01  2518.523016  599.135634  264.458043  2364.981209  5220.389769   \n",
      "1979-06-01  2927.137756  395.267754  202.659087  1864.819393  5602.199907   \n",
      "1979-07-01  1203.617911  397.443606  191.854634  1727.152076  3006.084426   \n",
      "1979-08-01   800.934150  383.599545  221.638224  1439.044599  2332.155432   \n",
      "...                 ...         ...         ...          ...          ...   \n",
      "2010-08-01  1537.159708  156.390801  133.331169  1160.518649  3422.201604   \n",
      "2010-09-01  2935.872420  281.306113  243.609570  2866.455046  7201.816009   \n",
      "2010-10-01  1620.451353  390.675933  200.661594  1651.151799  3658.468471   \n",
      "2010-11-01  2178.428373  477.138485  278.313391  2092.985524  4640.400044   \n",
      "2010-12-01  1106.622457  311.816611  284.541208  2060.855935  2527.814144   \n",
      "\n",
      "               er_pcp_l     on_pcp_l      mh_pcp_l    su_evap_w   er_evap_w  \\\n",
      "1979-04-01  4203.192412  3347.040605  10925.668866   339.531752   85.201176   \n",
      "1979-05-01  3569.127451  2594.097601   9747.586888   -25.832286  110.579591   \n",
      "1979-06-01  2959.050909  2041.493178   8656.297711  -266.550944  157.268096   \n",
      "1979-07-01  2688.125768  1775.973933   7640.559787   -73.502131  206.435721   \n",
      "1979-08-01  2628.734590  2562.512042   5033.708980   540.957328  258.450231   \n",
      "...                 ...          ...           ...          ...         ...   \n",
      "2010-08-01   830.806721  2028.928838   4696.342518   663.180124  359.738536   \n",
      "2010-09-01  1571.877797  2885.989172  10068.495380  1048.561240  442.253856   \n",
      "2010-10-01  1775.198627  2299.525719   5910.038960  1275.570387  418.297569   \n",
      "2010-11-01  3236.291721  2539.089961   6993.751048  1505.226102  291.818125   \n",
      "2010-12-01  1740.726215  2638.480677   6145.291828  1981.464961  326.894299   \n",
      "\n",
      "            ...  su_tmp_l_lag3  er_tmp_l_lag1  er_tmp_l_lag2  er_tmp_l_lag3  \\\n",
      "1979-04-01  ...          255.1          274.4          263.5          265.7   \n",
      "1979-05-01  ...          255.9          277.9          274.4          263.5   \n",
      "1979-06-01  ...          266.1          284.9          277.9          274.4   \n",
      "1979-07-01  ...          271.4          290.5          284.9          277.9   \n",
      "1979-08-01  ...          278.4          292.9          290.5          284.9   \n",
      "...         ...            ...            ...            ...            ...   \n",
      "2010-08-01  ...          282.8          296.2          293.4          288.6   \n",
      "2010-09-01  ...          286.3          295.8          296.2          293.4   \n",
      "2010-10-01  ...          291.7          290.7          295.8          296.2   \n",
      "2010-11-01  ...          291.7          285.0          290.7          295.8   \n",
      "2010-12-01  ...          283.2          278.0          285.0          290.7   \n",
      "\n",
      "            on_tmp_l_lag1  on_tmp_l_lag2  on_tmp_l_lag3  mh_tmp_l_lag1  \\\n",
      "1979-04-01          274.0          260.4          265.6         270.65   \n",
      "1979-05-01          277.7          274.0          260.4         275.25   \n",
      "1979-06-01          284.5          277.7          274.0         281.80   \n",
      "1979-07-01          289.6          284.5          277.7         288.20   \n",
      "1979-08-01          293.1          289.6          284.5         292.15   \n",
      "...                   ...            ...            ...            ...   \n",
      "2010-08-01          294.7          290.5          286.8         294.05   \n",
      "2010-09-01          293.7          294.7          290.5         294.00   \n",
      "2010-10-01          289.1          293.7          294.7         287.30   \n",
      "2010-11-01          282.8          289.1          293.7         282.50   \n",
      "2010-12-01          276.5          282.8          289.1         275.80   \n",
      "\n",
      "            mh_tmp_l_lag2  mh_tmp_l_lag3  \n",
      "1979-04-01         259.80         262.00  \n",
      "1979-05-01         270.65         259.80  \n",
      "1979-06-01         275.25         270.65  \n",
      "1979-07-01         281.80         275.25  \n",
      "1979-08-01         288.20         281.80  \n",
      "...                   ...            ...  \n",
      "2010-08-01         289.70         285.50  \n",
      "2010-09-01         294.05         289.70  \n",
      "2010-10-01         294.00         294.05  \n",
      "2010-11-01         287.30         294.00  \n",
      "2010-12-01         282.50         287.30  \n",
      "\n",
      "[381 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "lagged_X = create_lagged_variables(X, lags=3)\n",
    "print (lagged_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               su_pcp_w    er_pcp_w    on_pcp_w     mh_pcp_w     su_pcp_l  \\\n",
      "1979-04-01  1833.850353  771.898492  361.687857  3439.840263  3900.078255   \n",
      "1979-05-01  2518.523016  599.135634  264.458043  2364.981209  5220.389769   \n",
      "1979-06-01  2927.137756  395.267754  202.659087  1864.819393  5602.199907   \n",
      "1979-07-01  1203.617911  397.443606  191.854634  1727.152076  3006.084426   \n",
      "1979-08-01   800.934150  383.599545  221.638224  1439.044599  2332.155432   \n",
      "...                 ...         ...         ...          ...          ...   \n",
      "2010-08-01  1537.159708  156.390801  133.331169  1160.518649  3422.201604   \n",
      "2010-09-01  2935.872420  281.306113  243.609570  2866.455046  7201.816009   \n",
      "2010-10-01  1620.451353  390.675933  200.661594  1651.151799  3658.468471   \n",
      "2010-11-01  2178.428373  477.138485  278.313391  2092.985524  4640.400044   \n",
      "2010-12-01  1106.622457  311.816611  284.541208  2060.855935  2527.814144   \n",
      "\n",
      "               er_pcp_l     on_pcp_l      mh_pcp_l    su_evap_w   er_evap_w  \\\n",
      "1979-04-01  4203.192412  3347.040605  10925.668866   339.531752   85.201176   \n",
      "1979-05-01  3569.127451  2594.097601   9747.586888   -25.832286  110.579591   \n",
      "1979-06-01  2959.050909  2041.493178   8656.297711  -266.550944  157.268096   \n",
      "1979-07-01  2688.125768  1775.973933   7640.559787   -73.502131  206.435721   \n",
      "1979-08-01  2628.734590  2562.512042   5033.708980   540.957328  258.450231   \n",
      "...                 ...          ...           ...          ...         ...   \n",
      "2010-08-01   830.806721  2028.928838   4696.342518   663.180124  359.738536   \n",
      "2010-09-01  1571.877797  2885.989172  10068.495380  1048.561240  442.253856   \n",
      "2010-10-01  1775.198627  2299.525719   5910.038960  1275.570387  418.297569   \n",
      "2010-11-01  3236.291721  2539.089961   6993.751048  1505.226102  291.818125   \n",
      "2010-12-01  1740.726215  2638.480677   6145.291828  1981.464961  326.894299   \n",
      "\n",
      "            ...  su_tmp_l_lag3  er_tmp_l_lag1  er_tmp_l_lag2  er_tmp_l_lag3  \\\n",
      "1979-04-01  ...          255.1          274.4          263.5          265.7   \n",
      "1979-05-01  ...          255.9          277.9          274.4          263.5   \n",
      "1979-06-01  ...          266.1          284.9          277.9          274.4   \n",
      "1979-07-01  ...          271.4          290.5          284.9          277.9   \n",
      "1979-08-01  ...          278.4          292.9          290.5          284.9   \n",
      "...         ...            ...            ...            ...            ...   \n",
      "2010-08-01  ...          282.8          296.2          293.4          288.6   \n",
      "2010-09-01  ...          286.3          295.8          296.2          293.4   \n",
      "2010-10-01  ...          291.7          290.7          295.8          296.2   \n",
      "2010-11-01  ...          291.7          285.0          290.7          295.8   \n",
      "2010-12-01  ...          283.2          278.0          285.0          290.7   \n",
      "\n",
      "            on_tmp_l_lag1  on_tmp_l_lag2  on_tmp_l_lag3  mh_tmp_l_lag1  \\\n",
      "1979-04-01          274.0          260.4          265.6         270.65   \n",
      "1979-05-01          277.7          274.0          260.4         275.25   \n",
      "1979-06-01          284.5          277.7          274.0         281.80   \n",
      "1979-07-01          289.6          284.5          277.7         288.20   \n",
      "1979-08-01          293.1          289.6          284.5         292.15   \n",
      "...                   ...            ...            ...            ...   \n",
      "2010-08-01          294.7          290.5          286.8         294.05   \n",
      "2010-09-01          293.7          294.7          290.5         294.00   \n",
      "2010-10-01          289.1          293.7          294.7         287.30   \n",
      "2010-11-01          282.8          289.1          293.7         282.50   \n",
      "2010-12-01          276.5          282.8          289.1         275.80   \n",
      "\n",
      "            mh_tmp_l_lag2  mh_tmp_l_lag3  \n",
      "1979-04-01         259.80         262.00  \n",
      "1979-05-01         270.65         259.80  \n",
      "1979-06-01         275.25         270.65  \n",
      "1979-07-01         281.80         275.25  \n",
      "1979-08-01         288.20         281.80  \n",
      "...                   ...            ...  \n",
      "2010-08-01         289.70         285.50  \n",
      "2010-09-01         294.05         289.70  \n",
      "2010-10-01         294.00         294.05  \n",
      "2010-11-01         287.30         294.00  \n",
      "2010-12-01         282.50         287.30  \n",
      "\n",
      "[381 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "X = lagged_X\n",
    "# Merge the features and targets to align with the dates\n",
    "# Drops the dates where we don't have CFS data \n",
    "merged_df = pd.merge(X, targets, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# Pull the target variables back out \n",
    "y = merged_df[['su_evap_y', 'su_precip', 'su_runoff', 'er_evap_y', 'er_precip', 'er_runoff',\n",
    "               'on_evap_y', 'on_precip', 'on_runoff', 'mh_evap_y', 'mh_precip', 'mh_runoff']]\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data sets. We could do it as a random 80/20 split\n",
    "but instead we set split the data set by date ranges. This can easily be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_start_date = '1979-01-01'\n",
    "train_end_date = '2004-12-01'\n",
    "# Testing dataset\n",
    "val_start_date = '2005-01-01'\n",
    "val_end_date = '2011-01-01'\n",
    "\n",
    "X_train = X[train_start_date:train_end_date]\n",
    "y_train = y[train_start_date:train_end_date]\n",
    "X_test = X[val_start_date:val_end_date]\n",
    "y_test = y[val_start_date:val_end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (309, 96)\n",
      "Shape of y_train: (309, 12)\n",
      "Shape of X_test: (72, 96)\n",
      "Shape of y_test: (72, 12)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to standardize the data from 0-1 before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 96)\n",
      "(72, 96)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = x_scaler.fit_transform(X_test)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.fit_transform(y_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Below we train different models using the same data and calculate the r squared values on the \n",
    "test data to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.8378957976902296\n",
      "Mean Squared Error: 0.16210420230977038\n"
     ]
    }
   ],
   "source": [
    "# Testing Different Kernels\n",
    "# Basic kernel using ConstantKernel: r2 = 0.8259\n",
    "#kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "\n",
    "# Matt's optimal kernel: r2 = 0.8321\n",
    "#kernel = 1.0 * Matern(nu=1.5) * RationalQuadratic()\n",
    "\n",
    "# Test to add a seasonality component: r2 = 0.8279\n",
    "period = 3.0  # Period of the season\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + ExpSineSquared(length_scale=1.0, periodicity=period, periodicity_bounds=(1e-2, 1e2))\n",
    "\n",
    "# Set up the model\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gpr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Save the trained model\n",
    "#joblib.dump(gpr, 'GP_trained_model.joblib')\n",
    "#joblib.dump(x_scaler, 'x_scaler.joblib')\n",
    "#joblib.dump(y_scaler, 'y_scaler.joblib')\n",
    "\n",
    "# Predictions\n",
    "y_pred, sigma = gpr.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "r_squared = r2_score(y_test_scaled, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.7063397700123103\n",
      "Mean Squared Error: 113003.8740504076\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Regressor Model: r2 = 0.7389\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'RF_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1869960.5000 - val_loss: 1924885.3750\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1916225.1250 - val_loss: 1924183.1250\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1858305.2500 - val_loss: 1923263.7500\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1906919.0000 - val_loss: 1921910.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1808742.3750 - val_loss: 1919836.3750\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1861943.6250 - val_loss: 1916625.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1873305.7500 - val_loss: 1911846.3750\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1854955.8750 - val_loss: 1904934.8750\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1896222.6250 - val_loss: 1895153.1250\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1842323.7500 - val_loss: 1881833.7500\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1802505.0000 - val_loss: 1864042.8750\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1803972.6250 - val_loss: 1841202.2500\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1701257.5000 - val_loss: 1812269.7500\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1720719.5000 - val_loss: 1776936.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1680347.5000 - val_loss: 1734519.5000\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1627102.5000 - val_loss: 1685280.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1623607.1250 - val_loss: 1628864.3750\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1520199.2500 - val_loss: 1566244.5000\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1471702.2500 - val_loss: 1498035.2500\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1420987.3750 - val_loss: 1425724.3750\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1287188.7500 - val_loss: 1352386.7500\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1285477.0000 - val_loss: 1277815.8750\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1192841.2500 - val_loss: 1205003.7500\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1146630.0000 - val_loss: 1136076.5000\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1036167.4375 - val_loss: 1073362.5000\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 984313.4375 - val_loss: 1016625.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 955366.3125 - val_loss: 965587.9375\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 873727.8125 - val_loss: 921142.1875\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 863150.5000 - val_loss: 882647.5000\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 851816.3125 - val_loss: 849597.1875\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 765737.9375 - val_loss: 821181.6875\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 739976.6250 - val_loss: 796817.7500\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 766755.8125 - val_loss: 775108.7500\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 724459.3125 - val_loss: 756846.8750\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 706804.5625 - val_loss: 741267.8125\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 729386.1875 - val_loss: 726729.6875\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 684470.3125 - val_loss: 714582.1875\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 705821.1250 - val_loss: 703541.0625\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 659861.1875 - val_loss: 693983.8750\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 680585.8125 - val_loss: 684687.8750\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 677535.5000 - val_loss: 675804.2500\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 638788.9375 - val_loss: 667831.3750\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 617241.3125 - val_loss: 660720.3125\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 634804.5000 - val_loss: 653726.8750\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 631810.1875 - val_loss: 647367.1250\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 637037.0625 - val_loss: 640872.4375\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 616633.8125 - val_loss: 634838.7500\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 617893.5625 - val_loss: 628957.0625\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 627797.6875 - val_loss: 622992.8750\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 583975.1250 - val_loss: 617339.0000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "The r squared value for the model is -0.15938745439052582\n",
      "Mean Squared Error: 602626.7667079141\n"
     ]
    }
   ],
   "source": [
    "## Neural Network: r2 = 0.4002\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(12)  # Number of targets\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Using mean squared error (mse) as the loss function\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model,'NN_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
