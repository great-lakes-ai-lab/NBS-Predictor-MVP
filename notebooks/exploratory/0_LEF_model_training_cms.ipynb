{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/19/2024\n",
    "\n",
    "This script reads in CFSR data from 1979 - 2010 and trains different machine learning\n",
    "models to target RNBS from GLCC across the 5 Great Lakes simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, RBF, Matern, RationalQuadratic\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where the CFSR and GLCC files are located\n",
    "dir = 'C:/Users/fitzpatrick/Desktop/Data/Input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_in_month(year, month):\n",
    "    # Number of days in the month\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    # Convert days to seconds\n",
    "    return num_days * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mm_to_m3_per_sec(df):\n",
    "\n",
    "    # Calculate the number of seconds for each month\n",
    "    df['seconds'] = df.apply(lambda row: seconds_in_month(int(row['year']), int(row['month'])), axis=1)\n",
    "\n",
    "    # Convert millimeters to meters\n",
    "    df['WaterErie_m3'] = df['WaterErie'] / 1000\n",
    "    df['WaterOntario_m3'] = df['WaterOntario'] / 1000\n",
    "    df['WaterSuperior_m3'] = df['WaterSuperior'] / 1000\n",
    "    df['WaterMichHuron_m3'] = (df['WaterMichigan'] + df['WaterHuron']) / 1000\n",
    "\n",
    "    # Convert the data to cubic meters per second\n",
    "    df['WaterErie_cms'] = df['WaterErie_m3'] / df['seconds']\n",
    "    df['WaterOntario_cms'] = df['WaterOntario_m3'] / df['seconds']\n",
    "    df['WaterSuperior_cms'] = df['WaterSuperior_m3'] / df['seconds']\n",
    "    df['WaterMichHuron_cms'] = df['WaterMichHuron_m3'] / df['seconds']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rnbs_data(csv_file):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by melting it from wide to long format, creating a date column,\n",
    "    converting it to a specific format, and renaming columns.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input DataFrame with columns ['Year', 'Month', ...].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The processed DataFrame with columns ['Date', 'RNBS'].\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file, skiprows=11)\n",
    "\n",
    "    # Melt DataFrame to long format\n",
    "    df_melted = df.melt(id_vars=['Year'], var_name='Month', value_name='Value')\n",
    "    \n",
    "    # Combine Year and Month to create Date in the format YYYYMM\n",
    "    df_melted['Date'] = df_melted['Year'].astype(str) + df_melted['Month'].str[1:4] + '01'  # Extract month abbreviation and concatenate\n",
    "\n",
    "    # Convert the Date column to datetime\n",
    "    df_melted['Date'] = pd.to_datetime(df_melted['Date'], format='%Y%b%d')\n",
    "\n",
    "    # Rename the 'Value' column to 'RNBS'\n",
    "    df_melted.rename(columns={'Value': 'RNBS'}, inplace=True)\n",
    "\n",
    "    # Drop the original 'Year' and 'Month' columns\n",
    "    df_final = df_melted.drop(columns=['Year', 'Month'])\n",
    "\n",
    "    # Reset the index\n",
    "    df_final = df_final.set_index('Date')\n",
    "\n",
    "    print(df_final)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in PCP data from CFSR\n",
    "data_1 = pd.read_csv(dir+'CFSR_APCP_Basin_Sums.csv',sep=',')\n",
    "\n",
    "## Read in EVAP data from CFSR\n",
    "data_2 = pd.read_csv(dir+'CFSR_EVAP_Basin_Sums.csv',sep=',')\n",
    "\n",
    "## Read in TMP data from CFSR\n",
    "data_3 = pd.read_csv(dir+'CFSR_TMP_Basin_Avgs.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Total Precipitation to cms\n",
    "data_1 = convert_mm_to_m3_per_sec(data_1)\n",
    "\n",
    "# Convert Total Evaporation to cms\n",
    "data_2 = convert_mm_to_m3_per_sec(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the GLCC RNBS CSVs file in [cms]\n",
    "\n",
    "https://www.greatlakescc.org/en/coordinating-committee-products-and-datasets/#:~:text=Monthly%20Residual%20Net%20Basin%20Supplies%3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RNBS\n",
      "Date               \n",
      "1900-01-01   -520.0\n",
      "1901-01-01  -1130.0\n",
      "1902-01-01   -890.0\n",
      "1903-01-01   -340.0\n",
      "1904-01-01   -110.0\n",
      "...             ...\n",
      "2020-12-01  -1600.0\n",
      "2021-12-01    980.0\n",
      "2022-12-01   -460.0\n",
      "2023-12-01    830.0\n",
      "2024-12-01 -99999.0\n",
      "\n",
      "[1500 rows x 1 columns]\n",
      "               RNBS\n",
      "Date               \n",
      "1900-01-01   -150.0\n",
      "1901-01-01    250.0\n",
      "1902-01-01   -130.0\n",
      "1903-01-01    460.0\n",
      "1904-01-01    940.0\n",
      "...             ...\n",
      "2020-12-01    650.0\n",
      "2021-12-01   1280.0\n",
      "2022-12-01    560.0\n",
      "2023-12-01    920.0\n",
      "2024-12-01 -99999.0\n",
      "\n",
      "[1500 rows x 1 columns]\n",
      "               RNBS\n",
      "Date               \n",
      "1900-01-01   -180.0\n",
      "1901-01-01    430.0\n",
      "1902-01-01   -230.0\n",
      "1903-01-01   1490.0\n",
      "1904-01-01   1590.0\n",
      "...             ...\n",
      "2020-12-01   2480.0\n",
      "2021-12-01   2140.0\n",
      "2022-12-01    650.0\n",
      "2023-12-01   2840.0\n",
      "2024-12-01 -99999.0\n",
      "\n",
      "[1500 rows x 1 columns]\n",
      "               RNBS\n",
      "Date               \n",
      "1900-01-01    770.0\n",
      "1901-01-01    660.0\n",
      "1902-01-01    770.0\n",
      "1903-01-01    350.0\n",
      "1904-01-01    340.0\n",
      "...             ...\n",
      "2020-12-01   1020.0\n",
      "2021-12-01   1440.0\n",
      "2022-12-01   1280.0\n",
      "2023-12-01   1700.0\n",
      "2024-12-01 -99999.0\n",
      "\n",
      "[1500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "sup_file = dir + 'LakeSuperior_MonthlyNetBasinSupply_1900to2024.csv'\n",
    "eri_file = dir + 'LakeErie_MonthlyNetBasinSupply_1900to2024.csv'\n",
    "mih_file = dir + 'LakeMichiganHuron_MonthlyNetBasinSupply_1900to2024.csv'\n",
    "ont_file = dir + 'LakeOntario_MonthlyNetBasinSupply_1900to2024.csv'\n",
    "\n",
    "sup_rnbs = process_rnbs_data(sup_file)\n",
    "eri_rnbs = process_rnbs_data(eri_file)\n",
    "mih_rnbs = process_rnbs_data(mih_file)\n",
    "ont_rnbs = process_rnbs_data(ont_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and testing. We set the features 'X' as total over lake\n",
    "precipitation, total over lake evaporation, and the average air temperature over each lake. The\n",
    "targets 'y' are RNBS for each lake simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            su_rnbs  er_rnbs  on_rnbs  mh_rnbs\n",
      "1979-01-01   -640.0    890.0   1700.0   1770.0\n",
      "1979-02-01   1140.0   1530.0    700.0   1920.0\n",
      "1979-03-01   4280.0   2640.0   3200.0  10210.0\n",
      "1979-04-01   5620.0   3090.0   3210.0  11910.0\n",
      "1979-05-01   7660.0   1410.0   1470.0   8090.0\n",
      "...             ...      ...      ...      ...\n",
      "2010-08-01   2330.0   -800.0    180.0   1110.0\n",
      "2010-09-01   2340.0  -1010.0    150.0   1000.0\n",
      "2010-10-01  -1040.0   -430.0    830.0  -2070.0\n",
      "2010-11-01    120.0   -100.0   1050.0   -630.0\n",
      "2010-12-01  -1630.0      0.0   1240.0   -130.0\n",
      "\n",
      "[384 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Features\n",
    "X = pd.DataFrame({\n",
    "    'su_pcp': data_1['WaterSuperior_cms'],\n",
    "    'er_pcp': data_1['WaterErie_cms'],\n",
    "    'on_pcp': data_1['WaterOntario_cms'],\n",
    "    'mh_pcp': data_1['WaterMichHuron_cms'], #data_1['WaterMichigan']+data_1['WaterHuron'], # add the sums\n",
    "    'su_evap': data_2['WaterSuperior_cms'],\n",
    "    'er_evap': data_2['WaterErie_cms'],\n",
    "    'on_evap': data_2['WaterOntario_cms'],\n",
    "    'mh_evap': data_2['WaterMichHuron_cms'], #data_2['WaterMichigan']+data_2['WaterHuron'], # add the sums\n",
    "    'su_tmp': data_3['WaterSuperior'],\n",
    "    'er_tmp': data_3['WaterErie'],\n",
    "    'on_tmp': data_3['WaterOntario'],\n",
    "    'mh_tmp': (data_3['WaterMichigan']+data_3['WaterHuron'])/2 # take the average temp\n",
    "})\n",
    "\n",
    "# Set the index by date\n",
    "X.set_index(pd.to_datetime(data_1[['year', 'month']].assign(day=1)), inplace=True)\n",
    "\n",
    "# Targets\n",
    "rnbs = pd.DataFrame({\n",
    "    'su_rnbs': sup_rnbs['RNBS'],\n",
    "    'er_rnbs': eri_rnbs['RNBS'],\n",
    "    'on_rnbs': ont_rnbs['RNBS'],\n",
    "    'mh_rnbs': mih_rnbs['RNBS']\n",
    "})\n",
    "\n",
    "# Set the index of the targets RNBS to be the same index as the individual RNBS datasets\n",
    "rnbs.index = sup_rnbs.index\n",
    "\n",
    "# Merge the features and targets to align with the dates\n",
    "# Drops the dates in the RNBS where we don't have CFS data \n",
    "merged_df = pd.merge(X, rnbs, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# Pull the target variables back out \n",
    "y = merged_df[['su_rnbs', 'er_rnbs', 'on_rnbs', 'mh_rnbs']]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data sets. We could do it as a random 80/20 split\n",
    "but instead we set split the data set by date ranges. This can easily be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_start_date = '1979-01-01'\n",
    "train_end_date = '2004-12-01'\n",
    "# Testing dataset\n",
    "val_start_date = '2005-01-01'\n",
    "val_end_date = '2011-01-01'\n",
    "\n",
    "X_train = X[train_start_date:train_end_date]\n",
    "y_train = y[train_start_date:train_end_date]\n",
    "X_test = X[val_start_date:val_end_date]\n",
    "y_test = y[val_start_date:val_end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (312, 12)\n",
      "Shape of y_train: (312, 4)\n",
      "Shape of X_test: (72, 12)\n",
      "Shape of y_test: (72, 4)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to standardize the data from 0-1 before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 12)\n",
      "(72, 12)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = x_scaler.fit_transform(X_test)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.fit_transform(y_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Below we train different models using the same data and calculate the r squared values on the \n",
    "test data to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.8320987950154999\n",
      "Mean Squared Error: 0.16790120498450012\n"
     ]
    }
   ],
   "source": [
    "# Testing Different Kernels\n",
    "# Basic kernel using ConstantKernel: r2 = 0.8259\n",
    "#kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "\n",
    "# Matt's optimal kernel: r2 = 0.8321\n",
    "kernel = 1.0 * Matern(nu=1.5) * RationalQuadratic()\n",
    "\n",
    "# Test to add a seasonality component: r2 = 0.8279\n",
    "#period = 3.0  # Period of the season\n",
    "#kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + ExpSineSquared(length_scale=1.0, periodicity=period, periodicity_bounds=(1e-2, 1e2))\n",
    "\n",
    "# Set up the model\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gpr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(gpr, 'GP_trained_model.joblib')\n",
    "joblib.dump(x_scaler, 'x_scaler.joblib')\n",
    "joblib.dump(y_scaler, 'y_scaler.joblib')\n",
    "\n",
    "# Predictions\n",
    "y_pred, sigma = gpr.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "r_squared = r2_score(y_test_scaled, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.7388005673888829\n",
      "Mean Squared Error: 879987.1633680556\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Regressor Model: r2 = 0.7389\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'RF_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8631859.0000 - val_loss: 8135449.5000\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8376133.0000 - val_loss: 8133447.5000\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8280033.5000 - val_loss: 8130782.5000\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9098262.0000 - val_loss: 8127111.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8604907.0000 - val_loss: 8122127.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8342619.5000 - val_loss: 8115297.5000\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8966826.0000 - val_loss: 8105810.5000\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8399102.0000 - val_loss: 8092927.5000\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8461241.0000 - val_loss: 8075730.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8615867.0000 - val_loss: 8052960.5000\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8745180.0000 - val_loss: 8023729.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7629629.5000 - val_loss: 7987365.5000\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8094898.0000 - val_loss: 7941498.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8910197.0000 - val_loss: 7884108.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8834674.0000 - val_loss: 7816244.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7578500.5000 - val_loss: 7738148.5000\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8016325.0000 - val_loss: 7645295.5000\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8174437.0000 - val_loss: 7536083.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7863646.0000 - val_loss: 7412005.5000\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7806596.0000 - val_loss: 7272043.5000\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7599051.5000 - val_loss: 7115976.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6923324.5000 - val_loss: 6945212.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7591709.0000 - val_loss: 6756845.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6978576.5000 - val_loss: 6554596.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6853976.0000 - val_loss: 6337472.5000\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6941994.0000 - val_loss: 6107697.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6487754.5000 - val_loss: 5874658.5000\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6064128.0000 - val_loss: 5632497.5000\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6215732.5000 - val_loss: 5383447.5000\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5819657.0000 - val_loss: 5127385.5000\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5599756.0000 - val_loss: 4876512.5000\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5064601.0000 - val_loss: 4630562.5000\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4619061.0000 - val_loss: 4390121.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4746874.0000 - val_loss: 4154908.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4474667.5000 - val_loss: 3933622.7500\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4173454.5000 - val_loss: 3725166.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4404041.5000 - val_loss: 3521896.5000\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3674360.0000 - val_loss: 3338819.5000\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3616855.2500 - val_loss: 3165575.2500\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3224094.2500 - val_loss: 3005477.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3567217.2500 - val_loss: 2843240.2500\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3178743.0000 - val_loss: 2694850.5000\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2780624.0000 - val_loss: 2561214.5000\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2689992.7500 - val_loss: 2432298.7500\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2687219.7500 - val_loss: 2311141.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2417844.0000 - val_loss: 2199926.2500\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2405721.5000 - val_loss: 2097895.5000\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2355515.5000 - val_loss: 2004144.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2090908.5000 - val_loss: 1921822.3750\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2199464.7500 - val_loss: 1843644.5000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "The r squared value for the model is 0.4002319574356079\n",
      "Mean Squared Error: 2182391.2610983904\n"
     ]
    }
   ],
   "source": [
    "## Neural Network: r2 = 0.4002\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(4)  # Number of targets\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Using mean squared error (mse) as the loss function\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model,'NN_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
