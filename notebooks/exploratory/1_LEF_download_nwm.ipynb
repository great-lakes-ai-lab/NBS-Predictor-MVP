{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This script downloads all of the long-range National Water Model forecasts, from TODAY going out 30 days, and calculates the total runoff into each of the lakes.\n",
        "\n",
        "The following files are required to run this script:\n",
        "- ER_link.csv\n",
        "- ON_link.csv\n",
        "- SU_link.csv\n",
        "- MIHU_link.csv\n",
        "\n",
        "The following is an example of the output. Output will be total runoff (cms) for each lake for each of the 4 ensemble NWM members.\n",
        "\n",
        "              Superior          Erie       Ontario      MichHuron\n",
        "       1  85160.098097  32324.209277  34796.599222  111990.107497\n",
        "       2  85504.198089  32454.819275  34937.199219  112442.617487\n",
        "       3  85113.598098  32306.559278  34777.599223  111928.957498\n",
        "       4  87987.298033  33397.329254  35951.799196  115708.027414\n",
        "\n"
      ],
      "metadata": {
        "id": "LpMHvqniZ0Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install needed libraries\n",
        "!pip install netCDF4"
      ],
      "metadata": {
        "id": "LxD8p7XtoO4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c1563e-e077-4e7e-ed74-a74469048b23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.6.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2024.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.25.2)\n",
            "Installing collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.3 netCDF4-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import netCDF4\n",
        "from netCDF4 import Dataset\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import calendar"
      ],
      "metadata": {
        "id": "fAOZS2g9lB64"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q-MY7vgptzm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476f2154-017f-45d6-edbb-e9d89d1cb719"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defined Functions\n",
        "\n",
        "Function to download the NWM forecasts."
      ],
      "metadata": {
        "id": "hnWP6B1mzrnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_nwm_forecast(forecast, ens_members, download_dir):\n",
        "    # Initialize a file counter\n",
        "    num_files_downloaded = 0\n",
        "\n",
        "    base_url = 'https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/v3.0/'\n",
        "    # This will need to be updated in the function to open using ftplib\n",
        "    base_url_fallback = 'ftp://ftpprd.ncep.noaa.gov/pub/data/nccf/com/nwm/'\n",
        "\n",
        "    dir = f'{download_dir}{YYYYMMDD}/'\n",
        "    # Create the download directory if it doesn't exist\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "    for ens in range(1,ens_members+1):\n",
        "\n",
        "      # Retrieve HTML content from the URL\n",
        "        path = base_url+f'nwm.{YYYYMMDD}/{forecast}_mem{ens}/'\n",
        "        files = f'nwm.t00z.{forecast}.channel_rt_{ens}'\n",
        "\n",
        "        response = urllib.request.urlopen(path)\n",
        "        html_content = response.read().decode('utf-8')\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        links = soup.find_all('a', href=lambda href: href and href.startswith(files))\n",
        "        for link in links:\n",
        "            file_url = path + link['href']\n",
        "            filename = link['href'].split('/')[-1]\n",
        "            file_path = os.path.join(dir, filename)\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "            num_files_downloaded += 1\n",
        "    print(f'Total number of NWM files downloaded: {num_files_downloaded}')"
      ],
      "metadata": {
        "id": "T9ZLNfbWnvr5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function currently being worked on that would use the FTP address as a backup if Nomads were to fail.\n",
        "import requests\n",
        "from ftplib import FTP\n",
        "\n",
        "def download_nwm_forecast_backup(forecast, ens_members, download_dir):\n",
        "    # Initialize a file counter\n",
        "    num_files_downloaded = 0\n",
        "\n",
        "    base_url = 'https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/v3.0/'\n",
        "    # This will need to be updated in the function to open using ftplib\n",
        "    ftp_address = 'ftp://ftpprd.ncep.noaa.gov/pub/data/nccf/com/nwm/'\n",
        "    ftp_path = '...'\n",
        "\n",
        "    # Create the download directory if it doesn't exist\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "\n",
        "    for ens in range(1,ens_members+1):\n",
        "\n",
        "      # Retrieve HTML content from the URL\n",
        "        path = base_url+f'nwm.{YYYYMMDD}/{forecast}_mem{ens}/'\n",
        "        files = f'nwm.t00z.{forecast}.channel_rt_{ens}'\n",
        "\n",
        "        try:\n",
        "        # Try to open the URL\n",
        "            response = requests.get(path)\n",
        "            response.raise_for_status()\n",
        "            return response.content\n",
        "        except requests.exceptions.RequestException:\n",
        "            # If URL access fails, switch to FTP\n",
        "            print(\"URL access failed, switching to FTP...\")\n",
        "            try:\n",
        "                ftp = FTP(ftp_address)\n",
        "                ftp.login()  # Log in to the FTP server\n",
        "                ftp.cwd(ftp_path)  # Change to the specified path\n",
        "                data = []\n",
        "                ftp.retrlines('RETR ' + ftp_path, data.append)  # Retrieve the file data\n",
        "                ftp.quit()  # Quit FTP connection\n",
        "                return '\\n'.join(data)\n",
        "            except Exception as e:\n",
        "                print(\"Failed to access FTP. Try again later.\")\n",
        "                return None\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        links = soup.find_all('a', href=lambda href: href and href.startswith(files))\n",
        "        for link in links:\n",
        "            file_url = path + link['href']\n",
        "            filename = link['href'].split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, filename)\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "            num_files_downloaded += 1\n",
        "    print(f'Total number of NWM files downloaded: {num_files_downloaded}')"
      ],
      "metadata": {
        "id": "nC3rTK_6XqDh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate RUNOFF into each lake from the NWM long range forecast."
      ],
      "metadata": {
        "id": "VGYtk5EXzzlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_runoff(ens_member, download_dir):\n",
        "\n",
        "    runoff_su, runoff_er, runoff_on, runoff_mh = 0, 0, 0, 0\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(download_dir):\n",
        "        if f'channel_rt_{ens_member}' in filename:\n",
        "\n",
        "            file = Dataset(download_dir+filename, mode='r', format=\"NETCDF4\")\n",
        "            feature_id = file.variables['feature_id']\n",
        "            streamflow = file.variables['streamflow'] #streamflow is in m3/s\n",
        "\n",
        "            for f_su in id_su:\n",
        "                runoff_su += streamflow[feature_id == f_su]\n",
        "            for f_er in id_er:\n",
        "                runoff_er += streamflow[feature_id == f_er]\n",
        "            for f_on in id_on:\n",
        "                runoff_on += streamflow[feature_id == f_on]\n",
        "            for f_mh in id_mh:\n",
        "                runoff_mh += streamflow[feature_id == f_mh]\n",
        "\n",
        "    return runoff_su, runoff_er, runoff_on, runoff_mh"
      ],
      "metadata": {
        "id": "1Lp4bI8kzpuK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loops through the above function using all the ensemble members and then puts the total runoff for each lake into an easy to read dataframe. It could probably be combined later with the above function."
      ],
      "metadata": {
        "id": "_pOF2c33fzFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_runoff_all(ens_members, directory):\n",
        "    ens = np.arange(1, ens_members + 1)\n",
        "    df_lakes = pd.DataFrame(index=ens, columns=lakes)\n",
        "    for i in range(1, ens_members+1):  # Assuming 4 columns for runoff values\n",
        "        runoff_values = calculate_runoff(i, directory)\n",
        "        for j, lake in enumerate(lakes):\n",
        "            df_lakes[lake][i] = runoff_values[j]\n",
        "    return df_lakes"
      ],
      "metadata": {
        "id": "l_EqwUbucjCz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is not used in this script anymore but wanted to leave it in here for future use in the src folder."
      ],
      "metadata": {
        "id": "XW2NPTtHgD5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cms_to_mm(flow_rate_cms, lake, month, year):\n",
        "    # Conversion factor from cubic meters per second to millimeters per month\n",
        "    # 1 m³/s * (60 * 60 * 24 * days_in_month) seconds/month = cubic meters per month\n",
        "    # To convert to millimeters per month, we divide by the lake area in square meters\n",
        "\n",
        "    # Determine lake area of the input lake\n",
        "    great_lakes_areas = {\n",
        "        \"Superior\": 82100,\n",
        "        \"Mich-Huron\": 57800+59600,\n",
        "        \"Erie\": 25700,\n",
        "        \"Ontario\": 19300\n",
        "    }\n",
        "    # Convert square kilometers to square meters\n",
        "    conversion_factor = 1000000\n",
        "\n",
        "    if lake in great_lakes_areas:\n",
        "        lake_area_m2 = great_lakes_areas[lake] * conversion_factor\n",
        "    else:\n",
        "        print(\"Lake input should be Superior, Erie, Ontario, or Mich-Huron.\")\n",
        "\n",
        "    # Determine number of days in the month using input month/year.\n",
        "    year = int(year)\n",
        "    month = int(month)\n",
        "    if month < 1 or month > 12:\n",
        "        raise ValueError(\"Month should be between 1 and 12.\")\n",
        "    # Get the number of days in the month\n",
        "    days_in_month = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    conversion_factor = (60 * 60 * 24 * days_in_month) / lake_area_m2\n",
        "\n",
        "    # Convert flow rate from m³/s to mm/month\n",
        "    total_mm = flow_rate_cms * conversion_factor\n",
        "\n",
        "    return total_mm"
      ],
      "metadata": {
        "id": "RBLvUXMtatkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin Script\n",
        "Preset Variables"
      ],
      "metadata": {
        "id": "_6sYGPZ05hI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab today's date for pulling the most current NWM forecast\n",
        "YYYYMMDD = datetime.today().strftime('%Y%m%d')\n",
        "\n",
        "# This will allow functions to be used to pull short_range, medium_range or long_range forecasts.\n",
        "forecast = 'long_range'\n",
        "\n",
        "# There are 4 ensemble members. If ens_members = 1, it will only pull the first ensember member.\n",
        "# If ens_members = 4, it will pull all 4 members.\n",
        "ens_members = 4\n",
        "\n",
        "# This allows you to specify a specific lake if not all are needed.\n",
        "lakes = ['Superior','Erie','Ontario','MichHuron']\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/BIL SA Project/Modeling/Inventories/Data/'\n",
        "\n",
        "# This is where you want to download the NWM files to.\n",
        "save_dir = '/content/drive/MyDrive/BIL SA Project/Modeling/Data-driven Modeling/Input datasets/Downloaded Data/'\n",
        "\n",
        "# Path you want it to save the output csv file to.\n",
        "output_path = '/content/drive/MyDrive/BIL SA Project/Modeling/Data-driven Modeling/Input datasets/'\n",
        "\n",
        "# Output filename name\n",
        "outfile = \"total_nwm_runoff_cms_\"+YYYYMMDD+\".csv\""
      ],
      "metadata": {
        "id": "7Q-ZfjnE4WAP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section reads in each of Yi's csv files that contain all of the feature_ids for the streams that discharge into each lake."
      ],
      "metadata": {
        "id": "9G9eQOI51iLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lake Superior\n",
        "su_links = pd.read_csv(input_dir+'NWM_runoff/SU_link.csv')\n",
        "id_su= su_links['ID']\n",
        "\n",
        "# Lake Erie\n",
        "er_links = pd.read_csv(input_dir+'NWM_runoff/ER_link.csv')\n",
        "id_er = er_links['ID']\n",
        "\n",
        "# Lake Ontario\n",
        "on_links = pd.read_csv(input_dir+'NWM_runoff/ON_link.csv')\n",
        "id_on = on_links['ID']\n",
        "\n",
        "# Lake Erie\n",
        "mh_links = pd.read_csv(input_dir+'NWM_runoff/MIHU_link.csv')\n",
        "id_mh = mh_links['ID']"
      ],
      "metadata": {
        "id": "6b1knAVJ46ay"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the NWM files"
      ],
      "metadata": {
        "id": "9rzrGQJkc3ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_nwm_forecast(forecast, ens_members, save_dir)"
      ],
      "metadata": {
        "id": "oyPFENCv2AdT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This opens each file and calculates the total runoff into each lake for each ensemble member (1-4)."
      ],
      "metadata": {
        "id": "kzIGt4z-c6by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example if you only wanted the runoff into each lake seperately.\n",
        "# su_runoff_1, er_runoff_1, on_runoff_1, mh_runoff_1 = calculate_runoff(1, save_dir)\n",
        "\n",
        "# This creates the dataframe with runoff into all the lakes for all the ensemble members.\n",
        "df_lakes = calculate_runoff_all(ens_members,save_dir)\n",
        "print(df_lakes)"
      ],
      "metadata": {
        "id": "gyY0szkwdbki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdd6b7d-ca37-4b03-b46b-6fa032b1a4b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Superior           Erie        Ontario      MichHuron\n",
            "1  439564.490175  166845.446271  179606.995985   578050.93708\n",
            "2  436820.990236  165804.096294  178485.996011   574443.08716\n",
            "3   441136.19014  167442.016257  180249.195971  580117.807033\n",
            "4  433231.190317  164441.516324  177019.196043  569722.307266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves a csv file with total flow into each lake [cms] for that time period\n",
        "df_lakes.to_csv(output_path+outfile, sep='\\t')"
      ],
      "metadata": {
        "id": "JD0jnbGvaRNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ignore All Below\n",
        "Below sections can be ignored. These sections were first drafted but accurately calculate runoff into each lake. I have kept them in here in order to compare to the output from the above functions."
      ],
      "metadata": {
        "id": "PerrCTfIicjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently ignore. This was a working section that I am writing into a function.\n",
        "# Keeping to test the function.\n",
        "hour = np.arange(6,721,6)\n",
        "members = 1,2,3,4\n",
        "\n",
        "# Set up empty data frames where the feature ids, pulled from the csvs above, are\n",
        "# the indices and there's an empty column to insert the streamflow for each file.\n",
        "df_su_hr = pd.DataFrame(index=id_su,columns=['streamflow'])\n",
        "df_er_hr = pd.DataFrame(index=id_er,columns=['streamflow'])\n",
        "df_on_hr = pd.DataFrame(index=id_on,columns=['streamflow'])\n",
        "df_mh_hr = pd.DataFrame(index=id_mh,columns=['streamflow'])\n",
        "\n",
        "# Empty data frame to insert the total runoff for each hour.\n",
        "df_su = pd.DataFrame(index=hour,columns=['streamflow'])\n",
        "df_er = pd.DataFrame(index=hour,columns=['streamflow'])\n",
        "df_on = pd.DataFrame(index=hour,columns=['streamflow'])\n",
        "df_mh = pd.DataFrame(index=hour,columns=['streamflow'])\n",
        "\n",
        "# Save each ensemble total streamflows for each lake.\n",
        "df_lakes = pd.DataFrame(index=members,columns=lakes)"
      ],
      "metadata": {
        "id": "cbrjsfd0T1eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently ignore. This was a working section that I am writing into a function.\n",
        "# Keeping to test the function.\n",
        "for ens in range(1,5):\n",
        "  #for hr in hour:\n",
        "  for hr in range(6,721,6):\n",
        "    filename = \"nwm.t00z.long_range.channel_rt_\"+str(ens)+\".f\"+str(hr).zfill(3)+\".conus.nc\"\n",
        "    file = Dataset(directory+filename, mode='r', format=\"NETCDF4\")\n",
        "    feature_id = file.variables['feature_id']\n",
        "    streamflow = file.variables['streamflow'] #streamflow is in m3/s\n",
        "\n",
        "    for f_su in id_su:\n",
        "      df_su_hr['streamflow'][f_su]= streamflow[feature_id == f_su]\n",
        "    for f_er in id_er:\n",
        "      df_er_hr['streamflow'][f_er]= streamflow[feature_id == f_er]\n",
        "    for f_on in id_on:\n",
        "      df_on_hr['streamflow'][f_on]= streamflow[feature_id == f_on]\n",
        "    for f_mh in id_mh:\n",
        "      df_mh_hr['streamflow'][f_mh]= streamflow[feature_id == f_mh]\n",
        "    file.close()\n",
        "\n",
        "    df_su['streamflow'][hr] = df_su_hr['streamflow'].sum()\n",
        "    df_er['streamflow'][hr] = df_er_hr['streamflow'].sum()\n",
        "    df_on['streamflow'][hr] = df_on_hr['streamflow'].sum()\n",
        "    df_mh['streamflow'][hr] = df_mh_hr['streamflow'].sum()\n",
        "    print(ens,hr,df_su['streamflow'][hr])\n",
        "\n",
        "  df_lakes[lakes[0]][ens] = df_su['streamflow'].sum()\n",
        "  df_lakes[lakes[1]][ens] = df_er['streamflow'].sum()\n",
        "  df_lakes[lakes[2]][ens] = df_on['streamflow'].sum()\n",
        "  df_lakes[lakes[3]][ens] = df_mh['streamflow'].sum()\n",
        "  print(ens,df_lakes[lakes[0]][ens])"
      ],
      "metadata": {
        "id": "XlpjpEE-ErCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_lakes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYqF3MZVXrBU",
        "outputId": "55585b33-123c-4acd-d5a4-dc0416227893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Superior           Erie        Ontario       MichHuron\n",
            "1  797205.282181  302595.123236  325739.792719  1048367.806567\n",
            "2  789941.982343  299838.193298  322771.992785  1038816.176781\n",
            "3  769454.082801  292061.603472  314400.592973  1011873.487383\n",
            "4  800469.582108  303834.153209  327073.592689  1052660.536471\n"
          ]
        }
      ]
    }
  ]
}